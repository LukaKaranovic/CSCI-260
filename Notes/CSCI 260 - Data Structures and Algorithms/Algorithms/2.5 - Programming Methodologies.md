## Brute Force
A brute force algorithm solves a problem by exhaustively enumerating through all of the possibilities.
Example: brute force string matching
## Greedy
A greedy algorithm always makes the (seemingly) best choice in each decision/iteration. Once a decision has been made, that decision is never reconsidered.
Example: Huffman encoding, Dijkstra's shortest path
## Backtracking
A backtracking algorithm systematically considers all possible outcomes for each decision, just like the brute force algorithms.

Backtracking algorithms can detect when an exhaustive search is unnecessary. Therefore, backtracking algorithms can perform much better than brute force ones when solving the same problem.
Example: depth first traversal
## Divide and Conquer
Falls into the first three categories, it consists of algorithms that divide a problem into several (usually smaller) sub-problems, solves each sub-problem independently, and then combines the solutions to obtain the solution for the original problem.
Example: quick sort; merge sort
## Dynamic Programming
Solves a series of sub-problems to solve the original problem. 

The series of sub-problems is devised carefully in such a way that each subsequent solution is obtained by combining the solutions to one or more of the sub-problems that have already been solved.

- Dynamic programming usually is used to solve optimization problems.
- The optimization problems usually have the following properties if dynamic programming techniques can be applied:
    - **Simple subproblems:** there has to be a simple way of repeatedly and recursively breaking the global optimization problem into subproblems.
    - **Subproblem optimality:** an optimal solution to the global problem must be a composition of optimal subproblem solutions.
    - **Subproblem overlap:** optimal solutions to unrelated subproblems can contain subproblems in common. This property keeps the total number of distinct subproblems small (usually is polynomial in the input size).
- The development of a dynamic programming algorithm can be broken into a sequence of the following steps:
    1. Characterize the structure of an optimal solution;
    2. Recursively define the value of an optimal solution;
    3. Compute the value of an optimal solution in a **bottom-up** fashion;
    4. Construct an optimal solution from computed information.
### Memoization (top-down):
* Cache previously computed results to use on next passes.
* Recursive implementation
* Good for small set of inputs
* Used when subproblems overlap
![[Pasted image 20241208144754.png]]

### Tabulation (bottom-up) 
* Solving all subproblems first and using their solutions to build up to the main problem.
* Iterative implementation
* Good for large set of inputs
* Used when subproblems do not overlap
![[Pasted image 20241208144836.png]]

## Randomization
Algorithm that has an element of randomness involved in solving a problem.

Example: coin-flipping technique in skip lists